# LLM 提供商

Koog 集成了主流的 LLM 提供商，并通过 [Ollama](https://ollama.com/) 支持本地模型。
目前支持以下提供商：

| <div style="width:115px">LLM 提供商</div> | 选择原因 |
|---------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|
| [OpenAI](https://platform.openai.com/docs/overview) (包括 [Azure OpenAI 服务](https://azure.microsoft.com/en-us/products/ai-foundry/models/openai)) | 具有广泛能力的高级模型。 |
| [Anthropic](https://www.anthropic.com/) | 长上下文和提示词缓存。 |
| [Google](https://ai.google.dev/) | 多模态处理（音频、视频）、大上下文。 |
| [DeepSeek](https://www.deepseek.com/) | 经济高效的推理和编码。 |
| [OpenRouter](https://openrouter.ai/) | 通过一次集成即可访问多个提供商的多种模型，以实现灵活性、提供商比较和统一的 API。 |
| [Amazon Bedrock](https://aws.amazon.com/bedrock/) | AWS 原生环境、企业级安全与合规性、多提供商访问。 |
| [Mistral](https://mistral.ai/) | 欧洲数据托管、GDPR 合规性。 |
| [阿里巴巴](https://www.alibabacloud.com/en?_p_lc=1) ([DashScope](https://dashscope.aliyun.com/) OpenAI 兼容客户端) | 大上下文和经济高效的通义千问模型。 |
| [Ollama](https://ollama.com/) | 隐私保护、本地开发、离线操作，且无 API 成本。 |

下表展示了 Koog 支持的 LLM 能力，以及哪些提供商的模型提供了这些能力。

| <div style="width:115px">LLM 能力</div> | OpenAI | Anthropic | Google | DeepSeek | OpenRouter | Amazon Bedrock | Mistral | 阿里巴巴 (DashScope OpenAI 兼容客户端) | Ollama (本地模型) |
|-----------------------------------------------|------------------------------|------------------------|--------------------------------------|----------|------------------|------------------|------------------------|----------------------------|-----------------------|
| 支持的输入 | 文本、图像、音频、文档 | 文本、图像、文档[^1] | 文本、图像、音频、视频、文档[^1] | 文本 | 因模型而异 | 因模型而异 | 文本、图像、文档[^1] | 文本、图像、音频、视频[^1] | 文本、图像[^1] |
| 响应流式传输 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| 工具 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓[^1] | ✓ | ✓ | ✓ |
| 工具选择 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓[^1] | ✓ | ✓ | – |
| 结构化输出 (JSON Schema) | ✓ | – | ✓ | ✓ | ✓[^1] | – | ✓ | ✓[^1] | ✓ |
| 多选 | ✓ | – | ✓ | – | ✓[^1] | ✓[^1] | ✓ | ✓[^1] | – |
| 温度 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| 推测 | ✓[^1] | – | – | – | ✓[^1] | – | ✓[^1] | ✓[^1] | – |
| 内容审核 | ✓ | – | – | – | – | ✓ | ✓ | – | ✓ |
| 嵌入 | ✓ | – | – | – | – | ✓ | ✓ | – | ✓ |
| 提示词缓存 | ✓[^1] | ✓ | – | – | – | – | – | – | – |
| 补全 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| 本地执行 | – | – | – | – | – | – | – | – | ✓ |

!!! note
    Koog 支持用于创建 AI 代理最常用的能力。
    各提供商的 LLM 可能拥有 Koog 当前不支持的额外特性。
    关于更多信息，请参考 [模型能力](model-capabilities.md)。

## 与提供商协作

Koog 允许您在两个层面与 LLM 提供商协作：

* 使用 **LLM 客户端**直接与特定提供商交互。
  每个客户端都实现了 `LLMClient` 接口，负责处理提供商的身份验证、
  请求格式化和响应解析。
  关于详细信息，请参见 [LLM 客户端](prompts/llm-clients.md)。

  * 使用 **提示词执行器**，它是一个更高级别的抽象，封装了一个或多个 LLM 客户端，
    管理它们的生命周期，并统一了跨提供商的接口。
    它可以在提供商之间切换，
    并可选择回退到使用对应客户端的已配置提供商和 LLM。
    您可以创建自己的执行器，也可以使用为特定提供商预定义的提示词执行器。
    关于详细信息，请参见 [提示词执行器](prompts/prompt-executors.md)。

使用提示词执行器提供一个更高级别的层，位于一个或多个 LLM 客户端之上。
它管理客户端生命周期，并公开一个跨提供商的统一接口。
在多提供商设置中，它可以在提供商之间路由请求，并可在核心请求需要时，选择回退到指定的客户端。
您可以创建自己的执行器，也可以使用预定义的执行器——包括单提供商和多提供商选项。

## 后续步骤

- [使用特定 LLM 提供商创建并运行代理](getting-started.md)。
- 深入了解 [提示词](prompts/index.md)。

[^1]: 该能力仅由提供商的部分模型支持。